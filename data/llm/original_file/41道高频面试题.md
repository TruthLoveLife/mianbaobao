# 41道高频面试题

1. 用语言介绍一下Transformer的总体流程
2. 深度学习的三种并行方式：数据并行，模型并行，流水线并行
3. Deepspeed分布式训练的理解，zero 0-3的理解
4. 对于CLIP的理解
5. 说几种对比学习的损失函数，以及它们的特点和优缺点
6. 说说大模型生成采样的几种方式，它们的特点和优缺点比较
7. 损失函数中温度的作用
8. BLIP的细节。（面试中提的问题是BLIP为什么将训练分成两个阶段）
9. Visual Encoder有哪些常见的类型？
10. 深度学习中常用的优化器有哪些？
11. SimCSE的理解
12. prenorm和postnorm
13. LLaMA 2的创新/ChatGLM的创新点/Qwen的创新点/Baichuan的创新点
14. LLM的评估方式有哪些？特点是什么？（中文的呢？）
15. 文本生成模型中生成参数的作用（temperature，top p，top k，num beams）
16. LoRA的作用和原理
17. CoT的作用
18. 神经网络经典的激活函数以及它们的优缺点
19. softmax函数求导的推导
20. BERT的参数量如何计算？
21. AUC和ROC
22. batch norm和layer norm
23. 大模型训练的超参数设置
24. 经典的词向量模型有哪些？
25. InstructGPT三个阶段的训练过程，用语言描述出来（过程，损失函数）
26. 大模型推理加速的方法
27. Transformer中注意力的作用是什么
28. RNN、CNN和Transformer的比较（复杂度，特点，适用范围等）
29. AC自动机
30. 产生梯度消失问题的原因有哪些？
31. 大模型的幻觉问题
32. 大模型训练数据处理
33. RLHF的计算细节
34. 构建CoT样本的时候，怎么保证覆盖不同的场景？
35. 回收的三个指标：Recall、NDCG、RMSE
36. RoPE和ALiBi
37. 交叉熵、NCE和InfoNCE的区别和联系
38. 贝叶斯学派和概率学派的区别
39. 一个文件的大小超过了主存容量，如何对这个文件进行排序？应该使用什么算法？
40. Python中的线程、进程和协程
41. python中的生成器和迭代器
